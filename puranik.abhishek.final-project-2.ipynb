{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSY 5336 001 Python Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN Moneyâ€™s Market Movers website (https://money.cnn.com/data/hotstocks/ )\n",
    "tracks the most active stocks on a real time basis. Specifically, the most active, the top gainers and\n",
    "top losers are listed at any instance in time. You will first write Python scripts that collect the list of\n",
    "most actives, gainers and losers from the above website. Next, your programs should take the ticker\n",
    "symbols and names of these companies (and categories) and build a csv file (called stocks.csv) with\n",
    "data about each stock from the website:\n",
    "https://finance.yahoo.com/quote/AMD?p=AMD&.tsrc=fin-srch-v1 which gives the quote for\n",
    "ticker symbol AMD as an example. \n",
    "\n",
    "The data to be collected from the Yahoo Finance site should\n",
    "include:\n",
    "OPEN price\n",
    "PREV CLOSE price\n",
    "VOLUME\n",
    "MARKET CAP\n",
    "Your code should also list the names of the companies in the order and categories listed in the\n",
    "website: https://money.cnn.com/data/hotstocks/ and ask the user to choose a company to get the data\n",
    "on. Once the user chooses the company of interest, your program should display its corresponding\n",
    "data (Open, Prev Close, Volume and Market Cap).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doltRLpv6pU_"
   },
   "source": [
    "\n",
    "\n",
    "    1.We need to install requests and beautifulsoup library.\n",
    "    2.Once installed, import the required libraries such as requests, regular expressions and beautifulsoup.\n",
    "    3.Create a new csv file named'stocks.csv' and open it in 'write' mode.\n",
    "    4.With the help of requests, enter url-https://money.cnn.com/data/hotstocks/' of which data needs to be scrapped.\n",
    "    5.For scrapping data of the web page, use beautifulsoup imported from library.\n",
    "    6.Create an empty list for storing the dictionary of abbreviations and company name.\n",
    "    7.Create a new list of most actives, gainers and losers companies in stocks.\n",
    "    8.Create a new function which would scrape data from the required web page-https://in.finance.yahoo.com/quote/.\n",
    "    9.Create a new dictionary having combination of ticker symbols and company name.\n",
    "    10.Store the data collected in the csv file.\n",
    "    11.Collect the data scrapped from the webpage using the defined function.Store the data in the csv file and close the           file.\n",
    "    12.Ask the user to enter the ticker symbol of the required company.(Ticker symbols are case sensitive)\n",
    "    13.The program again performs the scrapping as above and fetches the information.\n",
    "    14.The file is then monitored by reading, stripping and splitting operations bye new line character.Further, the data          is splitted according to the commas in the data.\n",
    "    15.The roubustness of the program is maintained by try and exception blocks. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.Install the required libraries.\n",
    "    2.First run, the cell that has the libraries and then the program cell.\n",
    "    3.Take the input from the user(Input is case sensitive)\n",
    "    4.Display the required information.\n",
    "    5.Error messages will be displayed if there is an issue with scrapping the webpage or the user inputs inaccurate ticker      symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.The ticker symbols are 'case sensitive'.\n",
    "    2.The program will function only once.If the user wants data for another company, run the program again.\n",
    "    3.If there are any unexpected errors, please restart kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The data for the T AT&T Inc is the following:\n",
    "\n",
    "    T AT&T Inc\n",
    "    Prev close: 37.66\n",
    "    Open:  37.74\n",
    "    Volume:  18,172,510\n",
    "    Market Cap:  273.061B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beautiful Soup is a library that makes it easy to scrape information from web pages. \n",
    "#It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tre\n",
    "\n",
    "\n",
    "#Requests is an elegant HTTP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in z:\\anaconda\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in z:\\anaconda\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in z:\\anaconda\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in z:\\anaconda\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in z:\\anaconda\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in z:\\anaconda\\lib\\site-packages (from requests) (2019.9.11)\n",
      "Requirement already satisfied: soupsieve>=1.2 in z:\\anaconda\\lib\\site-packages (from beautifulsoup4) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "V_vpSdDz6pW0",
    "outputId": "00aa2843-3462-4585-acbb-60051ae407e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project\n",
      "Which stock are you interested in:\n",
      "\n",
      "\n",
      " Most Actives: \n",
      "\n",
      "GE General Electric Co\n",
      "BAC Bank of America Corp\n",
      "T AT&T Inc\n",
      "AMD Advanced Micro Devices Inc\n",
      "F Ford Motor Co\n",
      "MSFT Microsoft Corp\n",
      "AAPL Apple Inc\n",
      "BMY Bristol-Myers Squibb Co\n",
      "FCX Freeport-McMoRan Inc\n",
      "HPQ HP Inc\n",
      "\n",
      "\n",
      " Gainers: \n",
      "\n",
      "NRG NRG Energy Inc\n",
      "HPQ HP Inc\n",
      "CF CF Industries Holdings Inc\n",
      "DXC DXC Technology Co\n",
      "CME CME Group Inc\n",
      "CRM Salesforce.Com Inc\n",
      "DRI Darden Restaurants Inc\n",
      "ICE Intercontinental Exchange Inc\n",
      "VRTX Vertex Pharmaceuticals Inc\n",
      "ATVI Activision Blizzard Inc\n",
      "\n",
      "\n",
      " Losers: \n",
      "\n",
      "APA Apache Corp\n",
      "FTI TechnipFMC PLC\n",
      "DVN Devon Energy Corp\n",
      "KSS Kohls Corp\n",
      "HP Helmerich and Payne Inc\n",
      "NBL Noble Energy Inc\n",
      "URI United Rentals Inc\n",
      "HBI HanesBrands Inc\n",
      "LYB LyondellBasell Industries NV\n",
      "EOG EOG Resources Inc\n",
      "\n",
      "Enter the required ticker symbol: T\n",
      "\n",
      "\n",
      "The data for the T AT&T Inc is the following:\n",
      "\n",
      "T AT&T Inc\n",
      "Prev close: 37.66\n",
      "Open:  37.74\n",
      "Volume:  18,172,510\n",
      "Market Cap:  273.061B\n"
     ]
    }
   ],
   "source": [
    "import requests                                        #importing requests, re and beautifulsoup from the libraries\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "f=open(\"stocks.csv\",'w+')                                 #Creating a new csv file and opening in 'write' mode\n",
    "   \n",
    "\n",
    "res=requests.get('https://money.cnn.com/data/hotstocks/')         #Using request, entering the reqd url to scrape data\n",
    "\n",
    "soup=BeautifulSoup(res.text,'html.parser')                        #Using the beautifulsoup for parsing \n",
    "repo=soup.find_all(class_='wsod_dataTable wsod_dataTableBigAlt')  #accessing the contents of the required class\n",
    "val_1 = [] \n",
    "val_2 = ['Most Actives:' , 'Gainers:' , 'Losers:']           #Creating a list of most actives, gainers and losers\n",
    "\n",
    "print('This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project')\n",
    "print('Which stock are you interested in:')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def  scrape():\n",
    "    \n",
    "    website= 'https://in.finance.yahoo.com/quote/'+i                  #Entering url of the second website which needs to be scrapped\n",
    "  \n",
    "    reqd_page1 = requests.get(website)  \n",
    "    soup1 = BeautifulSoup(reqd_page1.text, 'html.parser')                   #Using the beautifulsoup for parsing  \n",
    "    posts=soup1.find_all(class_=\"Bxz(bb) D(ib) Va(t) Mih(250px)--lgv2 W(100%) Mt(-6px) Mt(0px)--mobp Mt(0px)--mobl W(50%)--lgv2 Mend(20px)--lgv2 Pend(10px)--lgv2\") \n",
    "                                                                       #accessing the contents of the required class\n",
    "    \n",
    "    \n",
    "    \n",
    "    for post in posts:\n",
    "    \n",
    "        a_1=post.find('td', attrs={'data-test': 'PREV_CLOSE-value'}).get_text()    #Acquring the particular text using .get_text()         \n",
    "        a_2=post.find('td', attrs={'data-test': 'OPEN-value'}).get_text()\n",
    "        a_3=post.find('td', attrs={'data-test': 'TD_VOLUME-value'}).get_text()\n",
    "        a_4=post.find('td', attrs={'data-test': 'MARKET_CAP-value'}).get_text()\n",
    "        a_3=re.sub(r'[^\\w\\s]','',a_3)                                               #The re.sub() function in the re module that is used to replace substrings. \n",
    "\n",
    "        return(a_1,a_2,a_3,a_4)\n",
    "\n",
    "\n",
    "for data in repo: \n",
    "    [tmp.extract() for tmp in data.find_all(class_ = \"wsod_aRight\")]           #findall() finds *all* the matches and returns them as a list of strings, with each string representing one match.\n",
    "    abbr = [a.text for a in data.find_all(class_= \"wsod_symbol\")] \n",
    "    name = [a.text for a in data.find_all(\"span\")] \n",
    "    val_1 += [dict(zip(abbr, name))]                                         #Creating a new dictionary with reqd abbreviations and names\n",
    "    \n",
    "\n",
    "reqd_data = dict(zip(val_2, val_1))  \n",
    "\n",
    "some=' '\n",
    "z=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:                                                              #using try-except block to catch errors while data scrapping\n",
    "    while z<=2:\n",
    "        x=reqd_data[val_2[z]]\n",
    "        \n",
    "        g=(val_2[z])\n",
    "        \n",
    "        print('\\n\\n',g,'\\n')\n",
    "        f.write(g)                                      #write() writes a string str to the file\n",
    "        f.write('\\n\\n')\n",
    "        for i in x:\n",
    "\n",
    "                    print(i,x[i])   \n",
    "\n",
    "                    \n",
    "                    e,p,t,h = scrape() \n",
    "                    if (e or p or t or h) is not None:\n",
    "                        \n",
    "                        outputs=i,',',x[i],',',e,',',p,',',t,',',h,'\\n'\n",
    "                      \n",
    "                        ss=some.join(outputs)        #The join() method is a string method and returns a string in which the elements of sequence have been joined by str separator\n",
    "\n",
    "\n",
    "                        f.write(ss)\n",
    "                      \n",
    "                    else:\n",
    "                        print(\"error\")\n",
    "\n",
    "        z=z+1\n",
    "    f.close()                                       #Python file method close() closes the opened file\n",
    "        \n",
    "\n",
    "    while True:\n",
    "        got_value = False\n",
    "\n",
    "        ticker_symbol=input(\"\\nEnter the required ticker symbol: \")               #User enters the required ticker symbol\n",
    "        urll_new= 'https://in.finance.yahoo.com/quote/'+ticker_symbol\n",
    "           \n",
    "        reqd_page2 = requests.get(urll_new)                           #\n",
    "        soups_new = BeautifulSoup(reqd_page2.text, 'html.parser')        #Using the beautifulsoup for parsing \n",
    "        reqd_posts=soups_new.find_all(class_=\"Bxz(bb) D(ib) Va(t) Mih(250px)--lgv2 W(100%) Mt(-6px) Mt(0px)--mobp Mt(0px)--mobl W(50%)--lgv2 Mend(20px)--lgv2 Pend(10px)--lgv2\") \n",
    "\n",
    "        for txt_1 in reqd_posts:\n",
    "                                                                            #Acquring the particular text using .get_text() \n",
    "            pc=txt_1.find('td', attrs={'data-test': 'PREV_CLOSE-value'}).get_text()\n",
    "            ov=txt_1.find('td', attrs={'data-test': 'OPEN-value'}).get_text()\n",
    "            v=txt_1.find('td', attrs={'data-test': 'TD_VOLUME-value'}).get_text()\n",
    "            mv=txt_1.find('td', attrs={'data-test': 'MARKET_CAP-value'}).get_text()\n",
    "\n",
    "        jazzz=open('stocks.csv')\n",
    "        info=jazzz.read().strip().split('\\n')                     #The read() method reads the whole file\n",
    "                                                                 #The split() method splits a string into a list\n",
    "                                                              #The strip() method removes any leading and trailing characters\n",
    "\n",
    "        for nn in info:\n",
    "            z1=nn.split(',')\n",
    "            \n",
    "            for uu in z1:\n",
    "                j1=uu.strip()\n",
    "\n",
    "                \n",
    "                if (j1==ticker_symbol):\n",
    "                    print('\\n')\n",
    "                    print(\"The data for the\",z1[0].strip(),z1[1].strip(),\"is the following:\\n\")\n",
    "                    print(z1[0].strip(),z1[1].strip())\n",
    "                    \n",
    "                    print('Prev close:',  pc)                            #Printing the required attributes\n",
    "                    print('Open: ', ov)\n",
    "                    print('Volume: ', v)\n",
    "                    print('Market Cap: ', mv)\n",
    "                    got_value = True\n",
    "                    break\n",
    "               \n",
    "            if j1==ticker_symbol:\n",
    "                break\n",
    "\n",
    "        if got_value:\n",
    "            break\n",
    "        else:\n",
    "            print (\"\\n Please enter accurate ticker symbol\")    #This message will be displayed if the user enters inaccurate ticker symbol\n",
    "except:\n",
    "    print(\"\\n Something went wrong! \")                          #This message will be displayed if there is an error in webpage scrapping.                              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIgF7UPv6pW4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDiqMFjj6pW6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N2Wil8W6pW9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yy56UUy66pXA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MYPROJECT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
